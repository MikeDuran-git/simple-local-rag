{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
    "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check if GPU is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check gpu access\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU: \"+torch.cuda.get_device_name(0) + \" is available\")\n",
    "else: #we are using cpu\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Document/Text Processing and Embedding Creation\n",
    "\n",
    "Ingredients:\n",
    "* PDF document of choice.\n",
    "* Embedding model of choice.\n",
    "\n",
    "Steps:\n",
    "1. Import PDF document.\n",
    "2. Process text for embedding (e.g. split into chunks of sentences).\n",
    "3. Embed text chunks with embedding model.\n",
    "4. Save embeddings to file for later use (embeddings will store on file for many years or until you lose your hard drive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF document\n",
    "pdf_path = \"greg_doucette_cookbook_2_0.pdf\"\n",
    "\n",
    "# Download PDF if it doesn't already exist\n",
    "if not os.path.exists(pdf_path):\n",
    "  print(\"File doesn't exist, go to the link in the README to download it.\")\n",
    "else:\n",
    "  print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
    "\n",
    "    # Other potential text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "# Open PDF and get lines/pages\n",
    "# Note: this only focuses on text, rather than images/figures etc\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        #text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number+1,  # adjust page numbers since our PDF starts on page 42\n",
    "                                 \"page_char_count\": len(text),\n",
    "                                 \"page_word_count\": len(text.split(\" \")),\n",
    "                                 \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages_and_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate the pages by recipe, table for fruits and table for vegetables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_contents = pages_and_texts[len(pages_and_texts)-19:len(pages_and_texts)-2]\n",
    "recipes = pages_and_texts[16:len(pages_and_texts)-21]\n",
    "vegetables_raw_and_legumes_servings_reference_table=table_of_contents[-3:-1]\n",
    "fruits_servings_reference_table=table_of_contents[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vegetables_raw_and_legumes_servings_reference_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fruits_servings_reference_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P_D_I_P_R database creation\n",
    "\n",
    "We create a dataset with the following columns:\n",
    "* The page Number\n",
    "* The directions list\n",
    "* The Ingredients list\n",
    "* The preparation time\n",
    "* The ready in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(recipes)\n",
    "df.head()\n",
    "df.to_csv('cookbook_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the empty content (they are images in the book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the rows that contain ingredients and Directions since everything else is plain explanation\n",
    "df = df[df.text.str.contains('Ingredients|Directions')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get now the elements we need from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_directions(text: str) -> str:\n",
    "    # List of separators to use in the search\n",
    "    separators = [\n",
    "        \"N u t r i t i o n\",\n",
    "        \"N o t e\",\n",
    "        \"N U T R I T I O N\",\n",
    "        \"N O T E\",\n",
    "        \"P R E P\",\n",
    "        \"T O T A L\",\n",
    "        \"V E G E T A R I A N\",\n",
    "        \"V E G A N\",\n",
    "        \"\\nIngredients\\n\",\n",
    "        \"R E A D Y\"\n",
    "    ]\n",
    "    \n",
    "    # Construct a regex pattern to match any of the separators\n",
    "    separator_pattern = r'|'.join([re.escape(separator) for separator in separators])\n",
    "    \n",
    "    # Remove the \"number.\\n\" -> \"number. \" since it is not useful\n",
    "    text = re.sub(r'(\\d+)\\.\\n', r'\\1. ', text)\n",
    "    \n",
    "    # Search for the section \"Directions\"\n",
    "    directions_match = re.search(r'Directions\\s*([\\s\\S]*?)\\s*(' + separator_pattern + r')', text, re.IGNORECASE)\n",
    "    if directions_match:\n",
    "        # Return the found directions\n",
    "        return directions_match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "#we test it on the first 5 rows\n",
    "df['directions'] = df.text.progress_apply(extract_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ingredients(text: str) -> str:\n",
    "    # List of separators to use in the search\n",
    "    separators = [\n",
    "        \"N u t r i t i o n\",\n",
    "        \"N o t e\",\n",
    "        \"N U T R I T I O N\",\n",
    "        \"N O T E\",\n",
    "        \"P R E P\",\n",
    "        \"T O T A L\",\n",
    "        \"V E G E T A R I A N\",\n",
    "        \"V E G A N\",\n",
    "        \"R E A D Y\",\n",
    "        \"C L I C K\",\n",
    "    ]\n",
    "    \n",
    "    # Construct a regex pattern to match any of the separators\n",
    "    separator_pattern = r'|'.join([re.escape(separator) for separator in separators])\n",
    "    \n",
    "    # Remove the \"number.\\n\" -> \"number. \" since it is not useful\n",
    "    text = re.sub(r'(\\d+)\\.\\n', r'\\1. ', text)\n",
    "    \n",
    "    # Search for the section \"Directions\"\n",
    "    ingredients_match = re.search(r'\\nIngredients\\n\\s*([\\s\\S]*?)\\s*(' + separator_pattern + r')', text, re.IGNORECASE)\n",
    "    if ingredients_match:\n",
    "        # Return the found ingredients\n",
    "        return ingredients_match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "df['ingredients'] = df.text.progress_apply(extract_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract prep time and ready in time\n",
    "def extract_prep_time_and_or_ready_in(text: str) -> dict:\n",
    "    prep_time = None\n",
    "    ready_in = None\n",
    "\n",
    "    # Find all occurrences of \"MINUTES\" and \"HOURS\" in uppercase along with their preceding numbers\n",
    "    times = re.findall(r'(\\d+)\\s*(MINUTES|HOUR)', text)\n",
    "\n",
    "    # Find the occurrence of \"P R E P  T I M E\" and \"R E A D Y  I N\"\n",
    "    prep_time_index = text.find(\"\\nP R E P  T I M E\")\n",
    "    ready_in_index = text.find(\"\\nR E A D Y  I N\")\n",
    "\n",
    "    if prep_time_index != -1 and ready_in_index != -1:\n",
    "        # Determine the order of prep time and ready in time based on their positions in the text\n",
    "        if prep_time_index < ready_in_index:\n",
    "            if len(times) > 0:\n",
    "                prep_time = times[0][0] + \" \" + times[0][1] if times[0][1] == \"MINUTES\" else None\n",
    "            if len(times) > 1:\n",
    "                ready_in = times[1][0] + \" \" + times[1][1]\n",
    "        else:\n",
    "            if len(times) > 0:\n",
    "                ready_in = times[0][0] + \" \" + times[0][1]\n",
    "            if len(times) > 1:\n",
    "                prep_time = times[1][0] + \" \" + times[1][1] if times[1][1] == \"MINUTES\" else None\n",
    "    elif prep_time_index != -1:\n",
    "        prep_time = times[0][0] + \" \" + times[0][1] if len(times) > 0 and times[0][1] == \"MINUTES\" else None\n",
    "    elif ready_in_index != -1:\n",
    "        ready_in = times[0][0] + \" \" + times[0][1] if len(times) > 0 else None\n",
    "\n",
    "    return {\n",
    "        \"prep_time\": prep_time,\n",
    "        \"ready_in\": ready_in\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df[['prep_time', 'ready_in']] = df['text'].apply(lambda x: pd.Series(extract_prep_time_and_or_ready_in(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified that \\u00b0F is Fahrenheit (°F) and \\u00b0C is Celcius (°C) So we replace it. And we know it is only in the direction, because, why would the degree too cook be in the ingredient list ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the \"ass\" in the dataset... because we are good boys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cookbook, there are notes and links that would not be necessary, as such we will remove it.\n",
    "It starts with \"click to...\", we don't need it, so we remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of phrases to remove\n",
    "click_to_and_get_strings = [\n",
    "    \"CLICK TO ORDER ICON MEALS PROTEIN BREAD\",\n",
    "    \"CLICK TO ORDER LOW-CALORIE SYRUP\",\n",
    "    \"CLICK TO PURCHASE GUAR GUM\",\n",
    "    \"CLICK TO ORDER WALDEN FARMS SYRUP\",\n",
    "    \"CLICK TO PURCHASE A NINJA BLENDER\",\n",
    "    \"CLICK TO PURCHASE MUSCLE EGG\",\n",
    "    \"CLICK TO PURCHASE LIQUID MUSCLE\",\n",
    "    \"CLICK TO PURCHASE MISSION CARB BALANCE TORTILLA\",\n",
    "    \"CLICK TO PURCHASE YVES VEGGIE TOFU DOGS\",\n",
    "    \"CLICK TO PURCHASE PALMINI LOW-CARB LASAGNA\",\n",
    "    \"CLICK TO PURCHASE VEGGIE GROUND \\\"MEAT\\\"\",\n",
    "    \"CLICK TO PURCHASE SUGAR-FREE CHOCOLATE SAUCE\",\n",
    "    \"GET SUGAR-FREE CHOCOLATE JELL-O PUDDING\",\n",
    "    \"GET CHOCOLATE SUGAR-FREE JELLO PUDDING MIX\",\n",
    "    \"GET GUAR GUM\",\n",
    "    \"GET PB2 POWDERED PEANUT BUTTER\",\n",
    "    \"CLICK TO PURCHASE PB2 POWDERED PEANUT BUTTER\",\n",
    "    \"CLICK TO PURCHASE PUMPKIN PURÉE\",\n",
    "    \"CLICK TO PURCHASE PB2 (POWDERED PEANUT BUTTER)\",\n",
    "    \"CLICK TO PURCHASE FIBER ONE BROWNIE BAR\",\n",
    "    \"CLICK TO PURCHASE CHOCOLATE PB2 POWDER\",\n",
    "    \"GET BANANA SUGAR-FREE JELLO PUDDING MIX\",\n",
    "    \"CLICK TO PURCHASE WALDEN FARMS MAPLE WALNUT SYRUP\",\n",
    "    \"CLICK TO PURCHASE HERSHEY'S HEALTH SHELL TOPPING\",\n",
    "    \"CLICK TO PURCHASE SUGAR-FREE JELLO CHEESECAKE PUDDING POWDER\",\n",
    "    \"CLICK TO PURCHASE LIBBY’S 100% PURE PUMPKIN\",\n",
    "    \"CLICK TO PURCHASE SUGAR-FREE VANILLA PUDDING JELL-O\",\n",
    "    \"GET CHOCOLATE PB2 POWDERED PEANUT BUTTER\",\n",
    "]\n",
    "\n",
    "# Function to remove specific phrases from text\n",
    "def remove_phrases(text, phrases):\n",
    "    for phrase in phrases:\n",
    "        text = text.replace(phrase, '')\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to all texts in the dataset\n",
    "df['text'] = df['text'].apply(lambda x: remove_phrases(x, click_to_and_get_strings))\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_file_path = 'cleaned_cookbook_data.csv'\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we replace de 1 hour by 60 minutes\n",
    "df['ready_in'] = df['ready_in'].str.replace('1 HOUR', '60 MINUTES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we replace the column name ready_in by ready_in_minutes and prep_time by prep_time_minutes\n",
    "df.rename(columns={'ready_in': 'ready_in_minutes', 'prep_time': 'prep_time_minutes'}, inplace=True)\n",
    "\n",
    "#we remove the words MINUTES and HOUR\n",
    "df['ready_in_minutes'] = df['ready_in_minutes'].str.replace('MINUTES', '')\n",
    "df['prep_time_minutes'] = df['prep_time_minutes'].str.replace('MINUTES', '')\n",
    "#we remove useless spaces\n",
    "df['ready_in_minutes'] = df['ready_in_minutes'].str.strip()\n",
    "df['prep_time_minutes'] = df['prep_time_minutes'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we drop the useless columns LIKE page_number, page_char_count, page_word_count, page_sentence_count_raw, page_token_count\n",
    "df.drop(columns=['text','page_char_count', 'page_word_count', 'page_sentence_count_raw', 'page_token_count'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('P_D_I_P_R.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We completed our first database. That contains the directions of the recipe, along with the ingredients, the page number, the prep time and the ready in time. Now we will focus on creating a second database that contains the recipes title, along with the macronutrients and the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  P_T_C_F_C_F_P database creation\n",
    "\n",
    "We create a dataset with the following columns:\n",
    "* The page Number\n",
    "* The title of the recipe\n",
    "* The calories\n",
    "* The Fat content\n",
    "* the Carb content\n",
    "* The fiber content\n",
    "* The Protein content\n",
    "\n",
    "We start of by getting the associated page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_recipe_nutrition_table = table_of_contents[:-4]\n",
    "master_recipe_nutrition_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do this page by page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the method to retreive the dataframe\n",
    "def get_dataframe_from_table_page(text):\n",
    "\n",
    "    #first off we remove the lines until we find the first occurence of \"Vegetarian\" and we drop it along with everything that came before him.\n",
    "    text = text[text.find(\"Vegetarian\"):].replace(\"Vegetarian\",\"\")\n",
    "\n",
    "    #when we see the pattern of any \\n next to \"-\" we remove the \"\\n\"\n",
    "    text = text.replace(\"\\n-\",\"-\").replace(\"\\n -\",\" -\").replace(\"-\\n\",\"- \").replace(\"\\n-\\n\",\"-\").replace(\"- \\n\",\"- \")\n",
    "\n",
    "    # if we see \\n next to serving or servings we remove it\n",
    "    text = re.sub(r'\\n(?=\\b[sS]erving[s]?\\b)', '', text)\n",
    "\n",
    "    # #we remove the first \\n\n",
    "    text = text.replace(\"\\n\",\" \",1)\n",
    "\n",
    "    #we remove \"\\nY\\n\"\n",
    "    text = text.replace(\"\\nY\",\"\")\n",
    "\n",
    "    array=text.split(\"\\n\")\n",
    "    #we remove unwanted spaces\n",
    "    array = [i.strip() for i in array]\n",
    "    #we remove empty strings\n",
    "    array = list(filter(None, array))\n",
    "\n",
    "    #on parcours le tableau, et nous calculons la longueur de chaque element. Si la le voisin i+1 est plus long que 3 c'est que c'est un text qui est coupé en deux, on le merge avec le voisin i. SAUF si i a une taille infieur ou égale à 3, car dans ce cas i est un nombre.\n",
    "    for i in range(len(array) - 1):\n",
    "        if len(array[i+1]) > 3 and len(array[i]) > 3 and not array[i+1].isnumeric() and not is_number_without_period(array[i]):\n",
    "            array[i] = array[i] + \" \" + array[i+1]\n",
    "            array[i+1] = \"\"\n",
    "\n",
    "    # we remove the empty strings\n",
    "    array = list(filter(None, array))\n",
    "    #we now that the first 7 elements are 1 recipe, so we can append by chunks of 7. The first element is the page, the second the recipe, the third the calories, the fourth the fat, the fifth the carbs, the sixth the fiber, the seventh the protein.\n",
    "\n",
    "    pages=[]\n",
    "    recipes=[]\n",
    "    calories=[]\n",
    "    fat=[]\n",
    "    carbs=[]\n",
    "    fiber=[]\n",
    "    protein=[]\n",
    "\n",
    "    for i in range(0,len(array),7):\n",
    "        pages.append(array[i])\n",
    "        recipes.append(array[i+1])\n",
    "        calories.append(array[i+2])\n",
    "        fat.append(array[i+3])\n",
    "        carbs.append(array[i+4])\n",
    "        fiber.append(array[i+5])\n",
    "        protein.append(array[i+6])\n",
    "        \n",
    "\n",
    "    #we create a dictionnary to store the data\n",
    "    nutrition_data = {\n",
    "        \"Page\": pages,\n",
    "        \"Recipe\": recipes,\n",
    "        \"Calories Per Serving\": calories,\n",
    "        \"Fat (g) per serving\": fat,\n",
    "        \"Carbs (g) per serving\": carbs,\n",
    "        \"Fiber (g) per serving\": fiber,\n",
    "        \"Protein (g) per serving\": protein,\n",
    "    }\n",
    "\n",
    "    #we create a dataframe\n",
    "    page_dataframe = pd.DataFrame(nutrition_data)\n",
    "\n",
    "    return page_dataframe\n",
    "\n",
    "\n",
    "# to remove . in the page number\n",
    "def is_number_without_period(s):\n",
    "    return s.replace(\".\", \"\").isnumeric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Pages of nutritional table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, page in enumerate(master_recipe_nutrition_table):\n",
    "    print(f\"Processing page {index}...\")\n",
    "    df = get_dataframe_from_table_page(page['text'])\n",
    "    df.to_csv(f'nutrition_table_{index}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the nutrition_table csv files and fuse them into one\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#we load the pages one by one\n",
    "page_1_dataframe = pd.read_csv('nutrition_table_0.csv')\n",
    "page_2_dataframe = pd.read_csv('nutrition_table_1.csv')\n",
    "page_3_dataframe = pd.read_csv('nutrition_table_2.csv')\n",
    "page_4_dataframe = pd.read_csv('nutrition_table_3.csv')\n",
    "page_5_dataframe = pd.read_csv('nutrition_table_4.csv')\n",
    "page_6_dataframe = pd.read_csv('nutrition_table_5.csv')\n",
    "page_7_dataframe = pd.read_csv('nutrition_table_6.csv')\n",
    "page_8_dataframe = pd.read_csv('nutrition_table_7.csv')\n",
    "page_9_dataframe = pd.read_csv('nutrition_table_8.csv')\n",
    "page_10_dataframe = pd.read_csv('nutrition_table_9.csv')\n",
    "page_11_dataframe = pd.read_csv('nutrition_table_10.csv')\n",
    "page_12_dataframe = pd.read_csv('nutrition_table_11.csv')\n",
    "page_13_dataframe = pd.read_csv('nutrition_table_12.csv')\n",
    "\n",
    "#we concatenate all the dataframes\n",
    "master_recipe_nutrition_table = pd.concat([page_1_dataframe, page_2_dataframe, page_3_dataframe, page_4_dataframe, page_5_dataframe, page_6_dataframe, page_7_dataframe, page_8_dataframe, page_9_dataframe, page_10_dataframe, page_11_dataframe, page_12_dataframe, page_13_dataframe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we order it by the page number\n",
    "P_T_C_F_C_F_P_dataframe = master_recipe_nutrition_table.sort_values(by='Page')\n",
    "P_T_C_F_C_F_P_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on our observation on the cookbook and the dataset. For unknown reason the page number was not respected starting from 48 became 49, and from then on, all numbers are +1, we simply remove this additionned value and reset the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from every row starting from 51 we reduce the page number by 1\n",
    "#we replace the page column by the new values\n",
    "for i in range(51,len(P_T_C_F_C_F_P_dataframe)):\n",
    "    P_T_C_F_C_F_P_dataframe.iloc[i,0] = int(P_T_C_F_C_F_P_dataframe.iloc[i,0])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_T_C_F_C_F_P_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we now need to do is is to manually set the vegan and vegetarian column, we can check in the cookbook each recipe to see if it is vegan or vegetarian. \n",
    "We can see that on page 187 there are all recipe are vegetaria (except 29,30,31) and all ar non vegan. So we can just write a python hard code that does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what we now need to do is is to manually set the vegan and vegetarian column, we can check in the cookbook each recipe to see if it is vegan or vegetarian. We can see that on page 187 there are all recipe are vegetaria (except 29,30,31) and all ar non vegan. So we can just write a python hard code that does that.\n",
    "\n",
    "#pages that are vegan:\n",
    "vegan_pages = [51,66,76,78,91,121,142,152,164,169,196,170,175]\n",
    "\n",
    "#we create the vegan column\n",
    "P_T_C_F_C_F_P_dataframe['Vegan'] = \"No\"\n",
    "\n",
    "#we set the vegan column to Yes for the pages that are vegan\n",
    "for page in vegan_pages:\n",
    "    P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Page'] == page, 'Vegan'] = \"Yes\"\n",
    "\n",
    "# we set the ChocolateProtein Mug Cake to no, because it is not vegan\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Recipe'] == 'Chocolate Protein Mug Cake', 'Vegan'] = \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we set the vegetarian column\n",
    "\n",
    "#we set every column as vegetarian\n",
    "P_T_C_F_C_F_P_dataframe['Vegetarian'] = \"Yes\"\n",
    "\n",
    "#now we just set to no the recipes that are not vegetarian\n",
    "non_vegetarian_pages=[29,30,31,\n",
    "                      \n",
    "                      63,64,68,70,\n",
    "                      \n",
    "                      75,\n",
    "                      80,\n",
    "                      85,\n",
    "                      86,87,89,90,92,93,98,99,101,102,\n",
    "\n",
    "                      105,107,108,110,111,113,115,116,118,\n",
    "\n",
    "                      126,\n",
    "\n",
    "                      152,\n",
    "\n",
    "                      ]\n",
    "\n",
    "for page in non_vegetarian_pages:\n",
    "    P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Page'] == page, 'Vegetarian'] = \"No\"\n",
    "\n",
    "\n",
    "#Egg White Wrap & Cauliflower PIzza Crust - Per 2 Meat Lovers Pizza is not vegetarian \n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Recipe'] == 'Cauliflower PIzza Crust - Per 2 Meat Lovers Pizza', 'Vegetarian'] = \"No\"\n",
    "# Cauliflower PIzza Crust - Per 2 Meat Lovers Pizza is not vegetarian\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Recipe'] == 'Cauliflower PIzza Crust - Per 2 Meat Lovers Pizza', 'Vegetarian'] = \"No\"\n",
    "# Sloppy Greg Sandwich - Total is not vegetarian\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Recipe'] == 'Sloppy Greg Sandwich - Total', 'Vegetarian'] = \"No\"\n",
    "# Sloppy Greg Sandwich - Per Serving is not vegetarian\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Recipe'] == 'Sloppy Greg Sandwich - Per Serving', 'Vegetarian'] = \"No\"\n",
    "# Grilled Chicken Wrap with Mango Relish - 1 Wrap is not vegetarian\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Recipe'] == 'Grilled Chicken Wrap with Mango Relish - 1 Wrap', 'Vegetarian'] = \"No\"\n",
    "# Egg Whites on Flatout Light OR La Tortilla OR 90-110 Calorie Wrap of Choice is vegetarian\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Recipe'] == 'Egg Whites on Flatout Light OR La Tortilla OR 90-110 Calorie Wrap of Choice', 'Vegetarian'] = \"Yes\"\n",
    "# Egg Whites on Joseph's Lavash is vegetarian\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Recipe'] == 'Egg Whites on Joseph\\'s Lavash', 'Vegetarian'] = \"Yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we succeded, we can create a category based on the table of content. \n",
    "* Breakfast: all pages from 17 to 60\n",
    "* Appetizer: all pages from 62 to 67\n",
    "* Tacos, Wraps and Sandwiches : all pages from 70 to 92\n",
    "* Dinner: 95 to 122\n",
    "* Treats: 125 to 156\n",
    "* Dessert: 158 to 185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we succeded, we can create a category based on the table of content. \n",
    "# * Breakfast: all pages from 17 to 60\n",
    "# * Appetizer: all pages from 62 to 67\n",
    "# * Tacos, Wraps and Sandwiches : all pages from 70 to 92\n",
    "# * Dinner: 95 to 122\n",
    "# * Treats: 125 to 156\n",
    "# * Dessert: 158 to 185\n",
    "\n",
    "#we create the category column\n",
    "P_T_C_F_C_F_P_dataframe['Category'] = \"Breakfast\"\n",
    "#we set the category column to the right category\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Page'].between(62, 67), 'Category'] = \"Appetizer\"\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Page'].between(70, 92), 'Category'] = \"Tacos, Wraps and Sandwiches\"\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Page'].between(95, 122), 'Category'] = \"Dinner\"\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Page'].between(125, 156), 'Category'] = \"Treats\"\n",
    "P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Page'].between(158, 186), 'Category'] = \"Dessert\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we save the dataframe\n",
    "P_T_C_F_C_F_P_dataframe.to_csv('P_T_C_F_C_F_P.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional, we can delete the nutrition_table csv files\n",
    "files = glob.glob('nutrition_table_*.csv')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Optimizations Possibilities:\n",
    "- On the PTCFCFP dataset, we need to correct the pages, some recipes do not exist and must be deleted.\n",
    "  - Ex: \"Hot Hamburg\",\"Ham and Cheese - Regular Ass White Bread\",\"Chicken Burger\" neither do \"PB2 and Jam Sandwhich - ICON\",\"PB2 and Jam Sandwhich - Ezekiel\",\"PB2 and Jam Sandwhich - Regular Ass White Bread\"\n",
    "- Add another of the Fruits and Vegetable dataset for better informations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get final dataset \"data\"\n",
    "We have now the 2 Databases to have all the informations of the recipes. What we can do now is potentially fuse them into one to have it in one single Dataset. We can create a Dataset that contains the following structure:\n",
    "* Page number\n",
    "* page_char_count\n",
    "* page_word_count\n",
    "* page_sentence_count_raw\n",
    "* page_token_count\n",
    "* Recipe information that is composed of the \"title\", \"ingredients\", \"directions\", \"Prep_and_ready_time_in_minutes\",\"Diet\" and \"category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we Creat a dataframe called data that contains the following columns: Page, Recipe\n",
    "\n",
    "#we have our 2 datasets P_D_I_P_R.csv and P_T_C_F_C_F_P.csv :\n",
    "\n",
    "#we load the dataframes\n",
    "P_D_I_P_R_dataframe = pd.read_csv('P_D_I_P_R.csv')\n",
    "P_T_C_F_C_F_P_dataframe = pd.read_csv('P_T_C_F_C_F_P.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_D_I_P_R_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we create a method that based on a certain page number, we return the recipe\n",
    "def get_recipe_from_page(page_number):\n",
    "\n",
    "    extract_directions = P_D_I_P_R_dataframe.loc[P_D_I_P_R_dataframe['page_number'] == page_number, 'directions'].values[0] \n",
    "\n",
    "    extract_ingredients = P_D_I_P_R_dataframe.loc[P_D_I_P_R_dataframe['page_number'] == page_number, 'ingredients'].values[0]\n",
    "\n",
    "    extracted_prep_time = P_D_I_P_R_dataframe.loc[P_D_I_P_R_dataframe['page_number'] == page_number, 'prep_time_minutes'].values[0]\n",
    "\n",
    "    extracted_ready_in = P_D_I_P_R_dataframe.loc[P_D_I_P_R_dataframe['page_number'] == page_number, 'ready_in_minutes'].values[0]\n",
    "\n",
    "    text = \"\"\"DIRECTIONS:\\n{}\\n\\nINGREDIENTS:\\n{}\\nPREP TIME (in m): {}\\nREADY IN (in m): {}\n",
    "    \"\"\".format(extract_directions, extract_ingredients, extracted_prep_time, extracted_ready_in)\n",
    "    return text\n",
    "\n",
    "# we get the title and macros from a certain page number\n",
    "def get_title_and_macros(page_number):\n",
    "\n",
    "    #since it is possible to have multiple values associated to a page number, we set them in an array\n",
    "    array = P_T_C_F_C_F_P_dataframe.loc[P_T_C_F_C_F_P_dataframe['Page'] == page_number].iloc[0:].to_dict('records')\n",
    "\n",
    "    text=\"\"\n",
    "\n",
    "    \n",
    "    # if the array equals to 1, that means that there is only one recipe on that page, so we can return the title and the macros\n",
    "    #if len(array) == 1:\n",
    "    for recipe in array:\n",
    "        title = recipe['Recipe']\n",
    "        calories = recipe['Calories Per Serving']\n",
    "        fat = recipe['Fat (g) per serving']\n",
    "        carbs = recipe['Carbs (g) per serving']\n",
    "        fiber = recipe['Fiber (g) per serving']\n",
    "        protein = recipe['Protein (g) per serving']\n",
    "        diet = \"vegan\" if recipe['Vegan'] == \"Yes\" else \"non-vegan\"\n",
    "        diet += \" , vegetarian\" if recipe['Vegetarian'] == \"Yes\" else \" , non-vegetarian\"\n",
    "\n",
    "        text+=\"\"\"TITLE: {}\\nCALORIES (in kcal): {}\\nFAT (in g): {}\\nCARBS (in g): {}\\nFIBER (in g): {}\\nPROTEIN (in g): {}\\nDIET: {}\\n\\n\"\"\".format(title, calories, fat, carbs, fiber, protein,diet)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "#We write a method to get all the informations of a recipe:\n",
    "def get_recipe(page_number):\n",
    "    recipe = get_recipe_from_page(page_number)\n",
    "    title_and_macros = get_title_and_macros(page_number)\n",
    "    return title_and_macros + recipe\n",
    "\n",
    "print(get_recipe(18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create the dataset \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_dataset = pd.DataFrame(columns=['page_number',\n",
    "                                       'page_char_count',\n",
    "                                        'page_word_count',\n",
    "                                        'page_sentence_count_raw',\n",
    "                                        'page_token_count',\n",
    "                                       'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_dataset['page_number'] = P_D_I_P_R_dataframe['page_number']\n",
    "\n",
    "recipe_dataset['text'] = recipe_dataset['page_number'].apply(get_recipe)\n",
    "recipe_dataset['page_char_count'] = recipe_dataset['text'].apply(lambda x: len(x))\n",
    "recipe_dataset['page_word_count'] = recipe_dataset['text'].apply(lambda x: len(x.split(\" \")))\n",
    "recipe_dataset['page_sentence_count_raw'] = recipe_dataset['text'].apply(lambda x: len(x.split(\". \")))\n",
    "                                                                         \n",
    "recipe_dataset['page_token_count'] = recipe_dataset['page_char_count'] / 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>1561</td>\n",
       "      <td>296</td>\n",
       "      <td>10</td>\n",
       "      <td>390.25</td>\n",
       "      <td>TITLE: Anabolic Apple Pie Breakfast Bake - Ent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>1251</td>\n",
       "      <td>205</td>\n",
       "      <td>11</td>\n",
       "      <td>312.75</td>\n",
       "      <td>TITLE: Anabolic French Toast - Per Serving\\nCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>1885</td>\n",
       "      <td>324</td>\n",
       "      <td>10</td>\n",
       "      <td>471.25</td>\n",
       "      <td>TITLE: Banana \"No\"Tella French Toast Roll-Ups ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>1408</td>\n",
       "      <td>235</td>\n",
       "      <td>12</td>\n",
       "      <td>352.00</td>\n",
       "      <td>TITLE: Blueberry French Toast - Large\\nCALORIE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>1546</td>\n",
       "      <td>295</td>\n",
       "      <td>10</td>\n",
       "      <td>386.50</td>\n",
       "      <td>TITLE: MEGA Peach French Toast Bake - Total\\nC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0           17             1561              296                       10   \n",
       "1           18             1251              205                       11   \n",
       "2           20             1885              324                       10   \n",
       "3           21             1408              235                       12   \n",
       "4           22             1546              295                       10   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0            390.25  TITLE: Anabolic Apple Pie Breakfast Bake - Ent...  \n",
       "1            312.75  TITLE: Anabolic French Toast - Per Serving\\nCA...  \n",
       "2            471.25  TITLE: Banana \"No\"Tella French Toast Roll-Ups ...  \n",
       "3            352.00  TITLE: Blueberry French Toast - Large\\nCALORIE...  \n",
       "4            386.50  TITLE: MEGA Peach French Toast Bake - Total\\nC...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>130.00</td>\n",
       "      <td>130.00</td>\n",
       "      <td>130.00</td>\n",
       "      <td>130.00</td>\n",
       "      <td>130.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>101.50</td>\n",
       "      <td>1376.47</td>\n",
       "      <td>232.18</td>\n",
       "      <td>10.03</td>\n",
       "      <td>344.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>50.41</td>\n",
       "      <td>471.78</td>\n",
       "      <td>82.18</td>\n",
       "      <td>3.59</td>\n",
       "      <td>117.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.00</td>\n",
       "      <td>422.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>105.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.25</td>\n",
       "      <td>1049.00</td>\n",
       "      <td>174.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>262.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>101.50</td>\n",
       "      <td>1344.00</td>\n",
       "      <td>224.50</td>\n",
       "      <td>10.00</td>\n",
       "      <td>336.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>144.75</td>\n",
       "      <td>1636.75</td>\n",
       "      <td>283.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>409.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>184.00</td>\n",
       "      <td>2515.00</td>\n",
       "      <td>431.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>628.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count       130.00           130.00           130.00                   130.00   \n",
       "mean        101.50          1376.47           232.18                    10.03   \n",
       "std          50.41           471.78            82.18                     3.59   \n",
       "min          17.00           422.00            71.00                     1.00   \n",
       "25%          55.25          1049.00           174.00                     8.00   \n",
       "50%         101.50          1344.00           224.50                    10.00   \n",
       "75%         144.75          1636.75           283.00                    12.00   \n",
       "max         184.00          2515.00           431.00                    20.00   \n",
       "\n",
       "       page_token_count  \n",
       "count            130.00  \n",
       "mean             344.12  \n",
       "std              117.95  \n",
       "min              105.50  \n",
       "25%              262.25  \n",
       "50%              336.00  \n",
       "75%              409.19  \n",
       "max              628.75  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we check the description of the dataset\n",
    "recipe_dataset.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looks like our average token count per page is 345.\n",
    "\n",
    "For this particular use case, it means we could embed an average whole page with the `all-mpnet-base-v2` model (this model has an input capacity of 384)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further text processing (splitting pages into sentences)\n",
    "\n",
    "We are going to decompose the strings into sentences:\n",
    "we want to follow the workflow of:\n",
    "\n",
    "`Ingest text -> split it into groups/chunks -> embed the groups/chunks -> use the embeddings`\n",
    "\n",
    "* Easier to handle than larger pages of text (especially if pages are densely filled with text).\n",
    "* Can get specific and find out which group of sentences were used to help within a RAG pipeline.\n",
    "> **Resource:** See [spaCy install instructions](https://spacy.io/usage). \n",
    "Let's use spaCy to break our text into sentences since it's likely a bit more robust than just using `text.split(\". \")`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
