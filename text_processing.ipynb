{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
    "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check if GPU is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU is available\n"
     ]
    }
   ],
   "source": [
    "#check gpu access\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU: \"+torch.cuda.get_device_name(0) + \" is available\")\n",
    "else: #we are using cpu\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Document/Text Processing and Embedding Creation\n",
    "\n",
    "Ingredients:\n",
    "* PDF document of choice.\n",
    "* Embedding model of choice.\n",
    "\n",
    "Steps:\n",
    "1. Import PDF document.\n",
    "2. Process text for embedding (e.g. split into chunks of sentences).\n",
    "3. Embed text chunks with embedding model.\n",
    "4. Save embeddings to file for later use (embeddings will store on file for many years or until you lose your hard drive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File greg_doucette_cookbook_2_0.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "# Get PDF document\n",
    "pdf_path = \"greg_doucette_cookbook_2_0.pdf\"\n",
    "\n",
    "# Download PDF if it doesn't already exist\n",
    "if not os.path.exists(pdf_path):\n",
    "  print(\"File doesn't exist, go to the link in the README to download it.\")\n",
    "else:\n",
    "  print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "205it [00:00, 517.36it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
    "\n",
    "    # Other potential text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "# Open PDF and get lines/pages\n",
    "# Note: this only focuses on text, rather than images/figures etc\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        #text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number+1,  # adjust page numbers since our PDF starts on page 42\n",
    "                                 \"page_char_count\": len(text),\n",
    "                                 \"page_word_count\": len(text.split(\" \")),\n",
    "                                 \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages_and_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate the pages by recipe, table for fruits and table for vegetables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_contents = pages_and_texts[len(pages_and_texts)-19:len(pages_and_texts)-2]\n",
    "recipes = pages_and_texts[16:len(pages_and_texts)-21]\n",
    "vegetables_raw_and_legumes_servings_reference_table=table_of_contents[-3:-1]\n",
    "fruits_servings_reference_table=table_of_contents[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vegetables_raw_and_legumes_servings_reference_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fruits_servings_reference_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe dataset creation\n",
    "\n",
    "We create a dataset with the following columns:\n",
    "* The page Number\n",
    "* The Ingredients list\n",
    "* The directions list\n",
    "* The preparation time\n",
    "* The ready in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(recipes)\n",
    "df.head()\n",
    "df.to_csv('cookbook_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the empty content (they are images in the book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the rows that contain ingredients and Directions since everything else is plain explanation\n",
    "df = df[df.text.str.contains('Ingredients|Directions')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17\\nwww.gregdoucette.com\\nT H E  U L T I M A T E  A N A B O L I C  C O O K B O O K  2 . 0\\nBack to Table of Contents\\nAnabolic Apple\\nPie Breakfast Bake\\nDirections\\n1.\\nPre-heat the oven to 400°F (204°C).\\n2.\\nChop the apples into small pieces.\\n3.\\nIn a bowl, whisk egg whites, cinnamon, sweetener, and vanilla.\\n4.\\nTear the bread into small pieces and place in a bowl with the egg\\nwhites, cinnamon, sweetener, and vanilla. Mix with your hands\\nuntil the bread pieces are well soaked with the batter.\\n5.\\nSpray a casserole dish with cooking spray for 1 second. Pour the\\negg white/bread mixture into the casserole dish.\\n6.\\nPlace the casserole dish uncovered in the middle rack and cook in\\nthe oven at 400°F/204°C for 40-50 minutes.\\nP R E P  T I M E\\nR E A D Y  I N\\n20 MINUTES\\n1 HOUR\\nIngredients\\nM A K E S  1  B A T C H .  S E R V I N G \\nS I Z E  V A R I E S  D E P E N D I N G \\nO N  H O W  L A R G E  O R  S M A L L \\nY O U  C U T  T H E  P I E C E S .\\n18 slices regular ass bread (or one \\nloaf [570g] of regular ass bread)\\n1920g (4 cartons/2000ml) egg \\nwhites\\n21g (3 tbsp) cinnamon\\n15g (1 tbsp) vanilla extract\\n15 packets (⅝ cup) sweetener\\n1500g or ~10 apples of your choice\\nCooking spray\\nN U T R I T I O N\\nP E R  S E R V I N G\\nC A L O R I E S\\nF A T  ( G )\\nC A R B S  ( G )\\nF I B E R  ( G )\\nE N T I R E\\nB A T C H\\n3 2 5 0\\n1 7\\n4 4 4\\n4 6\\nL A R G E\\nS E R V I N G\\n5 4 0\\n3\\n7 4\\n8\\nP R O T E I N  ( G )  \\n2 6 5  \\n4 4\\nS M A L L\\nS E R V I N G\\n2 7 0\\n1\\n3 7\\n4\\n2 2\\nN O T E :  T H I S  R E C I P E  E I T H E R \\nM A K E S\\n6  L A R G E  S E R V I N G S  O R\\n1 2  S M A L L  S E R V I N G S .\\nD I V I D E  T H E  B A T C H  I N T O \\nP O R T I O N S  A C C O R D I N G  T O \\nY O U R  P R E F E R R E D  S E R V I N G \\nS I Z E  A N D  D A I L Y  C A L O R I E \\nR E Q U I R E M E N T S .\\nN O T E :  T H I S  H A S  M O R E  E G G  W H I T E S  A N D \\nM O R E  F R U I T ,  M A K I N G  I T  H E A L T H I E R  A N D \\nL E S S  C A L O R I E  D E N S E  T H A N  L A S T  T I M E ! \\nC O A C H  G R E G  L O V E S  T H I S  D I S H  W I T H \\nG R A N N Y  S M I T H  A P P L E S ,  B U T  Y O U  C A N  U S E \\nW H I C H E V E R  A P P L E S  Y O U  W A N T .  T H E R E  A R E \\nN O  R U L E S  I N  T H I S  K I T C H E N ! !\\nV E G E T A R I A N\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get now the elements we need from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:00<00:00, 4990.71it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_directions(text: str) -> str:\n",
    "    # List of separators to use in the search\n",
    "    separators = [\n",
    "        \"N u t r i t i o n\",\n",
    "        \"N o t e\",\n",
    "        \"N U T R I T I O N\",\n",
    "        \"N O T E\",\n",
    "        \"P R E P\",\n",
    "        \"T O T A L\",\n",
    "        \"V E G E T A R I A N\",\n",
    "        \"V E G A N\",\n",
    "        \"\\nIngredients\\n\",\n",
    "        \"R E A D Y\"\n",
    "    ]\n",
    "    \n",
    "    # Construct a regex pattern to match any of the separators\n",
    "    separator_pattern = r'|'.join([re.escape(separator) for separator in separators])\n",
    "    \n",
    "    # Remove the \"number.\\n\" -> \"number. \" since it is not useful\n",
    "    text = re.sub(r'(\\d+)\\.\\n', r'\\1. ', text)\n",
    "    \n",
    "    # Search for the section \"Directions\"\n",
    "    directions_match = re.search(r'Directions\\s*([\\s\\S]*?)\\s*(' + separator_pattern + r')', text, re.IGNORECASE)\n",
    "    if directions_match:\n",
    "        # Return the found directions\n",
    "        return directions_match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "#we test it on the first 5 rows\n",
    "df['directions'] = df.text.progress_apply(extract_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:00<00:00, 12583.01it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_ingredients(text: str) -> str:\n",
    "    # List of separators to use in the search\n",
    "    separators = [\n",
    "        \"N u t r i t i o n\",\n",
    "        \"N o t e\",\n",
    "        \"N U T R I T I O N\",\n",
    "        \"N O T E\",\n",
    "        \"P R E P\",\n",
    "        \"T O T A L\",\n",
    "        \"V E G E T A R I A N\",\n",
    "        \"V E G A N\",\n",
    "        \"R E A D Y\",\n",
    "        \"C L I C K\",\n",
    "    ]\n",
    "    \n",
    "    # Construct a regex pattern to match any of the separators\n",
    "    separator_pattern = r'|'.join([re.escape(separator) for separator in separators])\n",
    "    \n",
    "    # Remove the \"number.\\n\" -> \"number. \" since it is not useful\n",
    "    text = re.sub(r'(\\d+)\\.\\n', r'\\1. ', text)\n",
    "    \n",
    "    # Search for the section \"Directions\"\n",
    "    ingredients_match = re.search(r'\\nIngredients\\n\\s*([\\s\\S]*?)\\s*(' + separator_pattern + r')', text, re.IGNORECASE)\n",
    "    if ingredients_match:\n",
    "        # Return the found ingredients\n",
    "        return ingredients_match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "df['ingredients'] = df.text.progress_apply(extract_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prep_time': None, 'ready_in': '3'}\n",
      "{'prep_time': '3', 'ready_in': '5'}\n",
      "{'prep_time': None, 'ready_in': '10'}\n",
      "{'prep_time': '20', 'ready_in': None}\n"
     ]
    }
   ],
   "source": [
    "# Function to extract prep time and ready in time\n",
    "def extract_prep_time_and_or_ready_in(text: str) -> dict:\n",
    "    prep_time = None\n",
    "    ready_in = None\n",
    "\n",
    "    # Find all occurrences of \"MINUTES\" and \"HOURS\" along with their preceding numbers\n",
    "    times = re.findall(r'(\\d+)\\s*(MINUTES|HOURS)', text)\n",
    "\n",
    "    # Find the occurrence of \"P R E P  T I M E\" and \"R E A D Y  I N\"\n",
    "    prep_time_index = text.find(\"\\nP R E P  T I M E\")\n",
    "    ready_in_index = text.find(\"\\nR E A D Y  I N\")\n",
    "\n",
    "    if prep_time_index != -1 and ready_in_index != -1:\n",
    "        # Determine the order of prep time and ready in time based on their positions in the text\n",
    "        if prep_time_index < ready_in_index:\n",
    "            prep_time = times[0][0] if len(times) > 0 else None\n",
    "            ready_in = times[1][0] if len(times) > 1 else None\n",
    "        else:\n",
    "            ready_in = times[0][0] if len(times) > 0 else None\n",
    "            prep_time = times[1][0] if len(times) > 1 else None\n",
    "    elif prep_time_index != -1:\n",
    "        prep_time = times[0][0] if len(times) > 0 else None\n",
    "    elif ready_in_index != -1:\n",
    "        ready_in = times[0][0] if len(times) > 0 else None\n",
    "\n",
    "    return {\n",
    "        \"prep_time\": prep_time,\n",
    "        \"ready_in\": ready_in\n",
    "    }\n",
    "#it has only ready in\n",
    "page_80_text=df[df.page_number==80].text.values[0]\n",
    "page_78_text=df[df.page_number==78].text.values[0]\n",
    "page_77_text=df[df.page_number==77].text.values[0]\n",
    "page_58_text=df[df.page_number==58].text.values[0]\n",
    "\n",
    "print(extract_prep_time_and_or_ready_in(page_80_text))\n",
    "print(extract_prep_time_and_or_ready_in(page_78_text))\n",
    "print(extract_prep_time_and_or_ready_in(page_77_text))\n",
    "print(extract_prep_time_and_or_ready_in(page_58_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'58\\nwww.gregdoucette.com\\nT H E  U L T I M A T E  A N A B O L I C  C O O K B O O K  2 . 0\\nBack to Table of Contents\\nStrawberry Peach \\nProtein Muffins\\nDirections\\n1.\\nPreheat the oven to 350°F (177°C). Place liners in a muffin\\ntin and spray them with cooking spray.\\n2.\\nIn a bowl, mix all the dry ingredients together well. In a\\nseparate bowl or a stand mixer, whip together the rest of\\nthe ingredients until smooth. Add the dry ingredients to\\nthe wet ingredients and mix until incorporated. Fold in the\\nstrawberries and peaches and mix gently with a spoon until\\nmixed.\\n3.\\nFill the muffin liners about ¾ of the way full with the batter.\\nBake the muffins in the oven for 25 minutes or until a\\ntoothpick comes out clean when you prick the muffins.\\n4.\\nWhile the muffins are in the oven, place the frosting\\ningredients in a bowl and mix with a fork until well-blended.\\n5.\\nRemove the muffins from the oven and allow to cool for 15\\nminutes before placing the icing on top and serving.\\nP R E P  T I M E\\nR E A D Y  I N\\n20 MINUTES\\n1 HOUR\\nIngredients\\nM A K E S  1 0  M U F F I N S\\n285g (1¼ cup) 0% fat Greek yogurt\\n180g (¾ cup) egg whites\\n30ml (2 tbsp) unsweetened almond milk\\n45ml (3 tbsp) unsweetened applesauce\\n2 packets (or 4 tsp) sweetener\\n5g (1 tsp) vanilla extract\\n160g (~1⅓ cups) oat flour\\n33g (1 scoop) vanilla whey protein \\npowder\\n2.5g (½ tsp) baking powder\\n2g (½ tsp) baking soda\\n40g fresh strawberries, slices\\n40g fresh peaches, chopped\\nF R O S T I N G :\\n30ml (2 tbsp) Swerve 0-Calorie Icing \\nsugar\\n8ml (½ tbsp) unsweetened almond milk\\nV E G E T A R I A N\\nG L U T E N - F R E E\\nC L I C K  T O  O R D E R  S W E R V E \\n0 - C A L O R I E  I C I N G  S U G A R\\nN U T R I T I O N\\nP E R  B A T C H\\nC A L O R I E S\\n1 0 1 0\\nF A T  ( G )\\n1 3\\nC A R B S  ( G )\\n1 3 5\\nF I B E R  ( G )\\n4\\nP R O T E I N  ( G )\\n9 1\\nP R O T E I N  ( G )\\n9\\nN U T R I T I O N\\nP E R  M U F F I N\\nC A L O R I E S\\n1 0 1\\nF A T  ( G )\\n1\\nC A R B S  ( G )\\n1 4\\nF I B E R  ( G )\\n0\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_58_text=df[df.page_number==58].text.values[0]\n",
    "page_58_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df['text']\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we get the page number of the recipes it is always the first 2-3 characters of the text\n",
    "page_numbers=[]\n",
    "for i in text:\n",
    "    page_numbers.append(i[:3])\n",
    "page_numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we remove the rows that have page_sentence_count_raw <5\n",
    "df = df[df['page_sentence_count_raw'] > 5]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cookbook_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the starting title \"www.gregdoucette.com T H E  U L T I M A T E  A N A B O L I C  C O O K B O O K  2 . 0 Back to Table of Contents\" along with all the text that remove before www.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text\n",
    "def clean_text(page):\n",
    "    marker = \"U L T I M A T E  A N A B O L I C  C O O K B O O K  2 . 0 Back to Table of Contents\"\n",
    "    text = page[\"text\"]\n",
    "    if marker in text:\n",
    "        text = text.split(marker, 1)[1]\n",
    "\n",
    "    #if text does not contain at the beginning \"Title\" we add it\n",
    "    if not text.startswith(\"Title\"):\n",
    "        text = \"Title\" + text\n",
    "    text.replace(\"\\u00b0F\", \"°F\").replace(\"\\u00b0C\", \"°C\").replace('regular ass bread','regular bread')\n",
    "    return text\n",
    "\n",
    "\n",
    "for page in pages_and_texts:\n",
    "    page[\"text\"] = clean_text(page)\n",
    "\n",
    "pages_and_texts[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified that \\u00b0F is Fahrenheit (°F) and \\u00b0C is Celcius (°C) So we replace it. And we know it is only in the direction, because, why would the degree too cook be in the ingredient list ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the \"ass\" in the dataset... because we are good boys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cookbook, there are notes and links that would not be necessary, as such we will remove it.\n",
    "It starts with \"click to...\", we don't need it, so we remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of phrases to remove\n",
    "click_to_and_get_strings = [\n",
    "    \"CLICK TO ORDER ICON MEALS PROTEIN BREAD\",\n",
    "    \"CLICK TO ORDER LOW-CALORIE SYRUP\",\n",
    "    \"CLICK TO PURCHASE GUAR GUM\",\n",
    "    \"CLICK TO ORDER WALDEN FARMS SYRUP\",\n",
    "    \"CLICK TO PURCHASE A NINJA BLENDER\",\n",
    "    \"CLICK TO PURCHASE MUSCLE EGG\",\n",
    "    \"CLICK TO PURCHASE LIQUID MUSCLE\",\n",
    "    \"CLICK TO PURCHASE MISSION CARB BALANCE TORTILLA\",\n",
    "    \"CLICK TO PURCHASE YVES VEGGIE TOFU DOGS\",\n",
    "    \"CLICK TO PURCHASE PALMINI LOW-CARB LASAGNA\",\n",
    "    \"CLICK TO PURCHASE VEGGIE GROUND \\\"MEAT\\\"\",\n",
    "    \"CLICK TO PURCHASE SUGAR-FREE CHOCOLATE SAUCE\",\n",
    "    \"GET SUGAR-FREE CHOCOLATE JELL-O PUDDING\",\n",
    "    \"GET CHOCOLATE SUGAR-FREE JELLO PUDDING MIX\",\n",
    "    \"GET GUAR GUM\",\n",
    "    \"GET PB2 POWDERED PEANUT BUTTER\",\n",
    "    \"CLICK TO PURCHASE PB2 POWDERED PEANUT BUTTER\",\n",
    "    \"CLICK TO PURCHASE PUMPKIN PURÉE\",\n",
    "    \"CLICK TO PURCHASE PB2 (POWDERED PEANUT BUTTER)\",\n",
    "    \"CLICK TO PURCHASE FIBER ONE BROWNIE BAR\",\n",
    "    \"CLICK TO PURCHASE CHOCOLATE PB2 POWDER\",\n",
    "    \"GET BANANA SUGAR-FREE JELLO PUDDING MIX\",\n",
    "    \"CLICK TO PURCHASE WALDEN FARMS MAPLE WALNUT SYRUP\",\n",
    "    \"CLICK TO PURCHASE HERSHEY'S HEALTH SHELL TOPPING\",\n",
    "    \"CLICK TO PURCHASE SUGAR-FREE JELLO CHEESECAKE PUDDING POWDER\",\n",
    "    \"CLICK TO PURCHASE LIBBY’S 100% PURE PUMPKIN\",\n",
    "    \"CLICK TO PURCHASE SUGAR-FREE VANILLA PUDDING JELL-O\",\n",
    "    \"GET CHOCOLATE PB2 POWDERED PEANUT BUTTER\",\n",
    "]\n",
    "\n",
    "# Function to remove specific phrases from text\n",
    "def remove_phrases(text, phrases):\n",
    "    for phrase in phrases:\n",
    "        text = text.replace(phrase, '')\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to all texts in the dataset\n",
    "df['text'] = df['text'].apply(lambda x: remove_phrases(x, click_to_and_get_strings))\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_file_path = '/mnt/data/cleaned_cookbook_data.csv'\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaced_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pages = []\n",
    "for page in pages_and_texts:\n",
    "    text = page['text']\n",
    "    cleaned_text = re.sub(r'CLICK TO.*', '', text)\n",
    "    cleaned_pages.append({'page_number': page['page_number'], 'page_char_count': len(cleaned_text), 'page_word_count': len(cleaned_text.split()), 'page_sentence_count_raw': len(cleaned_text.split('. ')), 'page_token_count': len(cleaned_text) / 4, 'text': cleaned_text})\n",
    "\n",
    "cleaned_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text = pages_and_texts[0][\"text\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Input text\n",
    "text = pages_and_texts[0][\"text\"]\n",
    "\n",
    "# Function to parse the recipe text\n",
    "def extract_directions_and_ingredients(text):\n",
    "    \n",
    "    # Extracting the directions\n",
    "    directions_match = re.search(r'Directions(.*?)P R E P  T I M E', text, re.DOTALL)\n",
    "    directions = directions_match.group(1).strip() if directions_match else \"No Directions Found\"\n",
    "    \n",
    "    # Extracting the ingredients\n",
    "    ingredients_match = re.search(r'Ingredients(.*?)N U T R I T I O N P E R  S E R V I N G', text, re.DOTALL)\n",
    "    ingredients = ingredients_match.group(1).strip() if ingredients_match else \"No Ingredients Found\"\n",
    "    \n",
    "    return {\n",
    "        \"directions\": directions,\n",
    "        \"ingredients\": ingredients\n",
    "    }\n",
    "\n",
    "# Parsing the provided text\n",
    "parsed_recipe = extract_directions_and_ingredients(text)\n",
    "\n",
    "# Displaying the parsed recipe\n",
    "parsed_recipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
